{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5f7c7a",
   "metadata": {},
   "source": [
    "Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c8cd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c1a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        hidden = self.fc(embedded.mean(dim=1))\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.out(hidden)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('./imdb.csv')\n",
    "\n",
    "# Splitting the dataset into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% training, 30% for val and test\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split the 30% into 15% val and 15% test\n",
    "\n",
    "# Cleaning Data\n",
    "def clean_text(text):\n",
    "    # Add whitespace around punctuation\n",
    "    text = re.sub(r'([.,!?()])', r' \\1 ', text)\n",
    "    # Remove non-punctuation symbols\n",
    "    text = re.sub(r'[^a-zA-Z.,!?() ]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to all splits\n",
    "train_df['cleaned_text'] = train_df['review'].apply(clean_text)\n",
    "val_df['cleaned_text'] = val_df['review'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nickparov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Map string labels to numeric values: 'negative' to 0 and 'positive' to 1\n",
    "label_map = {'negative': 0, 'positive': 1}\n",
    "train_df['sentiment'] = train_df['sentiment'].map(label_map).astype(float)\n",
    "val_df['sentiment'] = val_df['sentiment'].map(label_map).astype(float)\n",
    "test_df['sentiment'] = test_df['sentiment'].map(label_map).astype(float)\n",
    "\n",
    "# Download NLTK tokenizer model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization\n",
    "tokenized_reviews = [word_tokenize(review.lower()) for review in train_df['cleaned_text']]\n",
    "val_tokenized = [word_tokenize(review.lower()) for review in val_df['cleaned_text']]\n",
    "test_tokenized = [word_tokenize(review.lower()) for review in test_df['cleaned_text']]\n",
    "\n",
    "# Build Vocabulary\n",
    "word_counts = Counter(word for review in tokenized_reviews for word in review)\n",
    "vocabulary = {word: i + 1 for i, (word, _) in enumerate(word_counts.most_common(10000))}\n",
    "\n",
    "# Convert Text to Integer Sequences\n",
    "reviews_int = [[vocabulary[word] for word in review if word in vocabulary] for review in tokenized_reviews]\n",
    "val_reviews_int = [[vocabulary[word] for word in review if word in vocabulary] for review in val_tokenized]\n",
    "test_reviews_int = [[vocabulary[word] for word in review if word in vocabulary] for review in test_tokenized]\n",
    "review_lengths = [len(review) for review in tokenized_reviews]\n",
    "\n",
    "# Find a suitable max length\n",
    "percentile = 95\n",
    "max_len = int(np.percentile(review_lengths, percentile))\n",
    "\n",
    "# Padding Sequences\n",
    "padded_reviews = pad_sequences(reviews_int, maxlen=max_len, padding='post', truncating='post')\n",
    "val_padded_reviews = pad_sequences(val_reviews_int, maxlen=max_len, padding='post', truncating='post')\n",
    "test_padded_reviews = pad_sequences(test_reviews_int, maxlen=max_len, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "vocab_size = 10000 + 1 # +1 for padding token\n",
    "embedding_dim = 100  # common choice for embedding dimension\n",
    "hidden_dim = 64 # mid-range for hidden dimension\n",
    "output_dim = 1 # binary\n",
    "\n",
    "# init model\n",
    "model = SentimentClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "\n",
    "train_inputs = torch.tensor(padded_reviews, dtype=torch.long)\n",
    "val_inputs = torch.tensor(val_padded_reviews, dtype=torch.long)\n",
    "test_inputs = torch.tensor(test_padded_reviews, dtype=torch.long)\n",
    "\n",
    "# labels\n",
    "train_labels = torch.tensor(train_df['sentiment'].values, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_df['sentiment'].values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_df['sentiment'].values, dtype=torch.float32)\n",
    "\n",
    "# Train DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test DataLoader\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 0.5962, Train Accuracy: 0.6543\n",
      "Epoch 1/8, Validation Loss: 0.3987, Validation Accuracy: 0.8391\n",
      "Epoch 2/8, Train Loss: 0.3382, Train Accuracy: 0.8609\n",
      "Epoch 2/8, Validation Loss: 0.3087, Validation Accuracy: 0.8735\n",
      "Epoch 3/8, Train Loss: 0.2734, Train Accuracy: 0.8897\n",
      "Epoch 3/8, Validation Loss: 0.2917, Validation Accuracy: 0.8751\n",
      "Epoch 4/8, Train Loss: 0.2434, Train Accuracy: 0.9019\n",
      "Epoch 4/8, Validation Loss: 0.2644, Validation Accuracy: 0.8925\n",
      "Epoch 5/8, Train Loss: 0.2209, Train Accuracy: 0.9130\n",
      "Epoch 5/8, Validation Loss: 0.2735, Validation Accuracy: 0.8865\n",
      "Epoch 6/8, Train Loss: 0.2054, Train Accuracy: 0.9206\n",
      "Epoch 6/8, Validation Loss: 0.3199, Validation Accuracy: 0.8607\n",
      "Epoch 7/8, Train Loss: 0.1924, Train Accuracy: 0.9247\n",
      "Epoch 7/8, Validation Loss: 0.2559, Validation Accuracy: 0.8967\n",
      "Epoch 8/8, Train Loss: 0.1808, Train Accuracy: 0.9301\n",
      "Epoch 8/8, Validation Loss: 0.2655, Validation Accuracy: 0.8944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss, total, correct = 0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2616, Test Accuracy: 0.9005\n",
      "Top 20 Positive Words:\n",
      "entire: 26.370708465576172\n",
      "kind: 25.498929977416992\n",
      "camera: 23.64904022216797\n",
      "credits: 21.489665985107422\n",
      "men: 20.365657806396484\n",
      "appropriate: 20.28223419189453\n",
      "page: 20.166799545288086\n",
      "giving: 20.06629753112793\n",
      "views: 19.575176239013672\n",
      "green: 19.46065902709961\n",
      "boyfriend: 19.23826026916504\n",
      "mysterious: 19.13362693786621\n",
      "beautiful: 19.074342727661133\n",
      "weapons: 18.98100471496582\n",
      "wasted: 18.938899993896484\n",
      "mean: 18.158227920532227\n",
      "sounds: 18.05463218688965\n",
      "loves: 17.826236724853516\n",
      "sea: 17.820823669433594\n",
      "insane: 17.72930908203125\n",
      "\n",
      "Top 20 Negative Words:\n",
      "sunday: 8.13501262664795\n",
      "producers: 8.122499465942383\n",
      "marshall: 8.121447563171387\n",
      "made: 8.105531692504883\n",
      "washing: 8.09601879119873\n",
      "stupid: 8.087364196777344\n",
      "accidentally: 8.057730674743652\n",
      "enemy: 8.051525115966797\n",
      "sigh: 8.036587715148926\n",
      "newcomer: 8.004074096679688\n",
      "shows: 7.994915008544922\n",
      "june: 7.987314224243164\n",
      "symphony: 7.984182357788086\n",
      "economic: 7.974999904632568\n",
      "thrill: 7.962036609649658\n",
      "alexandra: 7.954861640930176\n",
      "misses: 7.931981086730957\n",
      "worries: 7.739133834838867\n",
      "multiple: 7.527898788452148\n",
      "predator: 6.8740081787109375\n"
     ]
    }
   ],
   "source": [
    "# Model Testing\n",
    "model.eval()\n",
    "test_loss, test_correct, test_total = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Extract the embedding layer weights\n",
    "embedding_weights = model.embedding.weight.data\n",
    "\n",
    "# Calculate the influence of each word\n",
    "word_influence = torch.norm(embedding_weights, dim=1)\n",
    "\n",
    "# Create a dictionary of words and their corresponding influence\n",
    "word_influence_dict = {word: influence.item() for word, influence in zip(vocabulary, word_influence)}\n",
    "\n",
    "# Sort words by influence\n",
    "sorted_words = sorted(word_influence_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 20 influential words for positive and negative reviews\n",
    "# higher values indicate positive influence and lower values indicate negative\n",
    "top_positive_words = sorted_words[:20]\n",
    "top_negative_words = sorted_words[-20:]\n",
    "\n",
    "# Print top 20 influential words for positive reviews\n",
    "print(\"Top 20 Positive Words:\")\n",
    "for word, influence in top_positive_words[:20]:\n",
    "    print(f\"{word}: {influence}\")\n",
    "\n",
    "# Print top 20 influential words for negative reviews\n",
    "print(\"\\nTop 20 Negative Words:\")\n",
    "for word, influence in top_negative_words[:20]:\n",
    "    print(f\"{word}: {influence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
