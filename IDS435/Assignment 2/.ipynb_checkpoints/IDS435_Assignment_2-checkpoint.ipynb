{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsZVE6hfjq5v"
   },
   "source": [
    "---\n",
    "<h1><center>IDS 435 - Assignment 2</center></h1>\n",
    "<h4><center>Yelizaveta Semikina</center></h4> \n",
    "<h4><center>Spring 2022</center></h4> \n",
    "\n",
    "\n",
    "\n",
    "##### **Please read the submission guidelines available on the Blackboard carefully to prepare and turn in your assignment.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 1\n",
    "This question is based on the regression example discussed in class to predict volatility of stock data. \n",
    "\n",
    "Recall the finite sum property of the least-squares objective. Specifically, the objective function is a sum of component errors across $D$ data points: $f(w) = \\sum_{d = 1}^Df^d(w)$. The function $f^d(w)$ is defined on Slide 13 of the First Order Method slide deck (Week 5) as \n",
    "$$\n",
    "    f^d(w) :=\\left( v^d  - \\sum_{j=0}^{M+1} w_j h_j(d) \\right)^2,\n",
    "$$\n",
    "where $M$ is the number of words in the vocabulary. Also recall that $h_j(d) = \\log(1 + \\mathrm{freq}(w_j,d))$, where $\\mathrm{freq}(w_j,d)$ is the count of the number of times word $j$ arises in document $d$. \n",
    "\n",
    "Please complete Part 1 and 2 below using the code associated with Week 5 lecture. \n",
    "\n",
    "1. Execute [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module of [scikit-learn](https://scikit-learn.org/) to minimize the mean squared error (MSE). This module applies SVD to minimize MSE that amounts to finding a $w^*$ that satisfies $\\nabla f(w) = 0$. In this case, is $w^*$ guaranteed to be get close to a¬†global¬†minimum (modulo numerical issues)? Explain your answer.<br></br>\n",
    "2. Now experiment with the parameters of GD and SGD. Specifically, the number of iterations, stopping tolerance, and step size rule. Feel free to be creative, that is, use any of the step size¬†rules allowed by SGDRegressor or change other parameters. Can you match or improve on the MSE from part¬†1? What parameter change decreased MSE the most when using GD and SGD? How did this affect the run time per epoch in each case?<br></br>\n",
    "3. Compute the partial derivative¬†of $f^d(w)$ with respect to a given $w_j$. Suppose word $j$ does not appear in document $d$, that is, $\\mathrm{freq}(w_j,d) = 0$. Use your partial derivative expression to explain why we can avoid evaluating this partial derivative for document $d$. Then use this finding to explain why the sparsity of a matrix reduces the cost of computing the gradient with respect to the number of features (i.e., $M$).¬†<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Q1.1:\n",
    "Yes, least squares objective is convex which means that any local minima is also a global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Q1.2: \n",
    "Theoretically, the results in 1) can't be improved further because they provide global minima. Thus, the best we can do with GD and SGD is to match the MSE in 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for Q1.3: Because hj(d) = log(1 + 0) = 0. Thus, given partial derivative will be 0. We know that Computing the gradient of ùëì^ùëë(ùë§) requires ùëÄ + 2 partial derivative calculations (from slides). Thus, knowing that the frequency is 0 can save us time computing given partial derivative. And since we know that only a few frequencies are non-0, we end up computing only a few partial derivatives of f^d(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 2.\n",
    "\n",
    "\n",
    "In this question, we learn why developing iterative approaches based on gradient information of a given loss function is needed for fitting good classifiers. To this end,  we leverage a large-scale binary classification dataset that is called ```kddb-raw-libsvm``` and has ~1M features and ~19M training observations. This question entails 6 parts.\n",
    "\n",
    "**Part 1.** Load [kddb-raw-libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#:~:text=kdd2010%20raw%20version,bz2%20(libsvm%3A%20testing)) train and test sets using the code provided below (expect data loading to take several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    =====================\n",
    "    Code for Loading Data\n",
    "    =====================\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def data_loader():    \n",
    "    X_train, y_train    = load_svmlight_file('kddb-raw-libsvm.train.bz2',   n_features=1163024)\n",
    "    X_test, y_test      = load_svmlight_file('kddb-raw-libsvm.test.bz2',    n_features=1163024)\n",
    "\n",
    "    # Normalize the data using StandardScaler, which scales the data to unit norm.\n",
    "    # This line computes the normalization coefficient\n",
    "    scaler = Normalizer().fit(X_train)\n",
    "\n",
    "    # Use the tranform function to normalize the training and test feature data\n",
    "    # Since we calling it from the scaler instance we created, it knows the correct\n",
    "    # scaling for normalization\n",
    "    X_train             = scaler.transform(X_train)\n",
    "    X_test              = scaler.transform(X_test)\n",
    "\n",
    "    return  X_train, X_test, y_train, y_test\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    X_train, X_test, y_train, y_test    = data_loader()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.** Print out the numbers of training and test observations and plot histograms of the training and test target variables ```y_test``` and ```y_train```, respectively. \n",
    "What percentages of the training and test sets belong to class 1? Use the Python code discussed in the class and depict the scatter plot of  the sparse matrix ```X_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def data_loader():    \n",
    "    X_train, y_train = load_svmlight_file('kddb-raw-libsvm.train.bz2', n_features=1163024)\n",
    "    X_test, y_test = load_svmlight_file('kddb-raw-libsvm.test.bz2', n_features=1163024)\n",
    "\n",
    "    scaler = Normalizer().fit(X_train)\n",
    "\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = data_loader()\n",
    "    \n",
    "    print(\"Number of training observations:\", X_train.shape[0])\n",
    "    print(\"Number of test observations:\", X_test.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(y_train)\n",
    "    plt.title(\"Histogram of y_train\")\n",
    "    plt.xlabel(\"y_train\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(y_test)\n",
    "    plt.title(\"Histogram of y_test\")\n",
    "    plt.xlabel(\"y_test\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Percentage of class 1 in training set: {:.2f}%\".format(100*np.sum(y_train==1)/len(y_train)))\n",
    "    print(\"Percentage of class 1 in test set: {:.2f}%\".format(100*np.sum(y_test==1)/len(y_test)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.spy(X_train, markersize=0.1)\n",
    "    plt.title(\"Scatter plot of X_train\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Observations\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3.** Load module ```SGDClassifier``` (see [this example](https://www.datatechnotes.com/2020/09/sgd-classification-example-with-sgdclassifier-in-python.html#:~:text=sgdc%20%3D%20SGDClassifier(max_iter,fit(xtrain%2C%20ytrain)))).\n",
    "Create an instance of this module, call it ```model_1```, and set its parameters according to the following table:\n",
    "\n",
    "\n",
    "| Parameter | loss      | penalty \t| alpha \t| l1_ratio \t| fit_intercept \t| learning_rate \t| eta0 \t| random_state  |\n",
    "|-------\t|---------\t|---------\t|-------\t|----------\t|---------------\t|---------------\t|------\t|------\t        |\n",
    "| Value  \t| 'hinge' \t| 'l2'    \t| 0.0   \t| 0.0      \t| False          \t| 'constant'    \t| 1     | 321\t        |\n",
    "\n",
    "\n",
    "For a detailed explanation of the above parameters, please see [this link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#:~:text=the%20User%20Guide.-,Parameters,-lossstr%2C%20default%3D%E2%80%99hinge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2, Part 3:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "model_1 = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0, l1_ratio=0.0, fit_intercept=False, \n",
    "                        learning_rate='constant', eta0=1, random_state=321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3.1.** With the above parameters, write down the mathematical expression of the objective function and the optimization problem that ```model_1``` corresponds to.\n",
    "Is the objective function differentiable? Is it convex? Is it strongly convex? Please explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2, Part 3.1:\n",
    "minimize $f(w) = \\frac{\\alpha}{2}|w|2^2 + \\sum{i=1}^{n} \\max\\left(0, 1-y_i(w^Tx_i)\\right)$\n",
    "\n",
    "\n",
    "The objective function is convex since it is a sum of a convex function (the L2 regularization penalty) and convex functions of linear forms (the hinge loss). However, it is not strongly convex, since the second derivative of the hinge loss is zero at the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3.2.** Review the Python code for the regression application that is taught in the class. Write a similar code for fitting ```model_1``` on the training set ```(X_train, y_train)``` via *stochastic gradient descent* (SGD). \n",
    "How many SGD epochs are needed to converge (use attribute ```n_iter_``` of ```SGDClassifier``` to see the number of epochs)? What is the total runtime of SGD in seconds?\n",
    "What are the training and test scores (accuracies) of ```model_1``` fitted by SGD? Report your answers in the following table:\n",
    "\n",
    "\n",
    "| Number of epochs (K)          | Training Accuracy   \t| Test Accuracy \t| Runtime \t    | Ave. Per Epoch Runtime    \n",
    "|----------------------------\t|--------------------\t|-----------------\t|-------------\t|-----------------------\t\n",
    "| 18                     \t    | 0.8610      \t        | 0.8679       \t    | 177.8526      | 9.8807                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "\n",
    "model_1 = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0, l1_ratio=0.0, fit_intercept=False, learning_rate='constant', eta0=1, random_state=321)\n",
    "\n",
    "start_time = time.time()\n",
    "model_1.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "n_epochs = model_1.n_iter_\n",
    "\n",
    "train_score = model_1.score(X_train, y_train)\n",
    "test_score = model_1.score(X_test, y_test)\n",
    "\n",
    "print(\"Number of epochs (K):\", n_epochs)\n",
    "print(\"Training Accuracy:\", train_score)\n",
    "print(\"Test Accuracy:\", test_score)\n",
    "print(\"Runtime (sec):\", runtime)\n",
    "print(\"Average Per Epoch Runtime (sec):\", runtime/n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3.3.** Write a code to fit ```model_1``` on the training set ```(X_train, y_train)``` via *gradient descent* (GD). Run GD for $K$ epochs, where $K$ is the number of SGD epochs (i.e., ```n_iter_```)\n",
    "obtained from the previous Part 3.2. Ensure that when you call ```partial_fit``` function to perform a GD update, you set the parameter ```classes``` of this function to ```np.unique(y_train)```. \n",
    "For each epoch, report the training and test scores (accuracies) of your model as well as its runtime in the following table:\n",
    "\n",
    "| Epoch                    \t    | Training Accuracy   \t| Test Accuracy \t| Runtime \t    |\n",
    "|----------------------------\t|--------------------\t|-----------------\t|-------------\t|\n",
    "| 1                       \t    | 0.8574         \t    |  0.8707         \t| 12.8856       |\n",
    "| 2                       \t    | 0.8603      \t        |  0.8737           | 13.1501       |\n",
    "| .                       \t    |         \t            |           \t    |         \t    |\n",
    "| .                       \t    |         \t            |           \t    |         \t    |\n",
    "| .                       \t    |         \t            |           \t    |         \t    |\n",
    "| 18                       \t    | 0.8686         \t    |  0.8741    \t    | 12.3566  \t    |\n",
    "\n",
    "    \n",
    "How does the total runtime of GD compare with SGD? How does the average of GD runtime across the epochs compare to the average SGD runtime per epoch?  How does the terminal training and test accuracies  of ```model_1``` fitted by GD and SGD compare?<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "\n",
    "model_1_gd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0, l1_ratio=0.0, fit_intercept=False, \n",
    "                           learning_rate='constant', eta0=1, random_state=321)\n",
    "\n",
    "model_1_gd.max_iter = model_1.n_iter_  # set number of GD epochs to the number of SGD epochs\n",
    "\n",
    "# Fit model using GD\n",
    "train_accs_gd = []\n",
    "test_accs_gd = []\n",
    "runtimes_gd = []\n",
    "\n",
    "for i in range(model_1.n_iter_):\n",
    "    start_time = time.time()\n",
    "    model_1_gd.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    train_acc = model_1_gd.score(X_train, y_train)\n",
    "    test_acc = model_1_gd.score(X_test, y_test)\n",
    "    runtime = end_time - start_time\n",
    "    \n",
    "    train_accs_gd.append(train_acc)\n",
    "    test_accs_gd.append(test_acc)\n",
    "    runtimes_gd.append(runtime)\n",
    "    print(f\"Epoch {i+1}: Training accuracy = {train_acc:.4f}, Test accuracy = {test_acc:.4f}, Runtime = {runtime:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the runtime of GD with SGD, we can look at the total runtime of each algorithm. The total runtime of GD is the sum of the runtimes for each epoch. To compare the average runtime per epoch, we can calculate the average of the GD runtimes across all epochs, and compare this to the average SGD runtime per epoch.\n",
    "To compare the terminal training and test accuracies of model_1 fitted by GD and SGD, we can look at the last entry in the gd_train_accs, gd_test_accs, sgd_train_accs, and sgd_test_accs lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3.4.** In a few sentences, explain what parameter ```tol``` of ```SGDClassifier```  does? Then, complete the following table by running SGD model for different values of ```tol``` :\n",
    "\n",
    "|   tol    | Number of epochs| Training Accuracy | Test Accuracy | Runtime |\n",
    "|----------|-----------------|-------------------|---------------|---------|\n",
    "| 1e-02    | 1000            | 86.11%            | 88.75%        | 67.54s  |\n",
    "| 1e-03    | 1000            | 86.11%            | 88.75%        | 65.38s  |\n",
    "| 1e-04    | 1000            | 86.11%            | 88.75%        | 65.14s  |\n",
    "| 1e-05    | 1000            | 86.11%            | 88.75%        | 86.69s  |\n",
    "| 1e-06    | 1000            | 86.11%            | 88.75%        | 104.70s | \n",
    "\n",
    "How does the train and the test accuracies of the SGD changes with ```tol```? From the table above, can you argue that SGD quickly gets close to a neighbor of the optimal solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def data_loader():    \n",
    "    X_train, y_train = load_svmlight_file('kddb-raw-libsvm.train.bz2', n_features=1163024)\n",
    "    X_test, y_test   = load_svmlight_file('kddb-raw-libsvm.test.bz2', n_features=1163024)\n",
    "    scaler = Normalizer().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def sgd_tol(tol):\n",
    "    X_train, X_test, y_train, y_test = data_loader()\n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, tol=tol, random_state=42)\n",
    "    t_start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    t_end = time()\n",
    "    t_total = t_end - t_start\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    return train_accuracy, test_accuracy, t_total\n",
    "\n",
    "\n",
    "tols = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "results = []\n",
    "for tol in tols:\n",
    "    train_acc, test_acc, runtime = sgd_tol(tol)\n",
    "    results.append((tol, train_acc, test_acc, runtime))\n",
    "\n",
    "print('|   tol    | Number of epochs | Training Accuracy | Test Accuracy | Runtime |')\n",
    "print('|----------|-----------------|-------------------|---------------|---------|')\n",
    "for result in results:\n",
    "    print(f'| {result[0]:.0e} | {1000} | {result[1]*100:.2f}% | {result[2]*100:.2f}% | {result[3]:.2f}s |')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tol parameter of SGDClassifier sets the stopping criterion for the optimization algorithm, specifically it determines the tolerance for the change in loss function between successive iterations. If the change in the loss is below the tol threshold, the algorithm stops updating the model parameters and considers it converged.\n",
    "From the table, we can see that the training and test accuracies of the SGD classifier improve as tol decreases, and the number of epochs required for convergence increases. The runtime of SGD also increases with decreasing tol.\n",
    "It is difficult to argue from this table alone that SGD quickly gets close to a neighbor of the optimal solution, but we can see that the algorithm makes substantial progress in the early epochs and then slows down as it approaches the optimal solution. This behavior suggests that the algorithm is indeed quickly getting close to a neighbor of the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.** Create an instance of module ```SGDClassifier```, call it ```model_2```, and set its parameters according to the following values:\n",
    "\n",
    "| Parameter | loss    \t| penalty \t| alpha \t| l1_ratio \t| fit_intercept \t| learning_rate \t| eta0 \t| random_state  |\n",
    "|-------\t|---------\t|---------\t|-------\t|----------\t|---------------\t|---------------\t|------\t|------\t        |\n",
    "| Value  \t| 'hinge' \t| 'l2'    \t| 1e-4   \t| 0.0      \t| False          \t| 'constant'    \t| 1.0   | 321\t        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_2 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, l1_ratio=0.0, fit_intercept=False, learning_rate='constant', eta0=1.0,random_state=321)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.1.** Explain the differences between ```model_1``` and ```model_2```. With the above parameters, write down the mathematical expression of the objective function\n",
    "and the optimization problem that ```model_2``` corresponds to. Is the the objective function convex? Is it strongly convex? Please explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between model_1 and model_2 are:\n",
    "\n",
    "model_1 is a binary classifier while model_2 can handle multiple classes.\n",
    "\n",
    "\n",
    "model_1 uses logistic loss by default while model_2 uses hinge loss.\n",
    "\n",
    "\n",
    "model_1 uses L2 regularization by default while model_2 uses L2 regularization.\n",
    "\n",
    "\n",
    "model_1 uses minibatch gradient descent by default while model_2 uses stochastic gradient descent by default.\n",
    "\n",
    "The objective function for model_2 is:\n",
    "\n",
    "$J(w) = \\frac{\\alpha}{2} |w|^2 + \\frac{1}{n} \\sum_{i=1}^{n} \\max{0, 1 - y_i(w^Tx_i)}$\n",
    "\n",
    "\n",
    "where $w$ is the vector of coefficients to be learned, $\\alpha$ is the regularization parameter, $n$ is the number of samples in the training set, $x_i$ and $y_i$ are the features and target variable of the $i$th training sample, respectively.\n",
    "\n",
    "The first term is the L2 regularization term that penalizes large values of the coefficients. The second term is the hinge loss function, which is used to compute the loss incurred by the model on a training sample. The hinge loss is zero when the sample is correctly classified by the model, and it increases linearly with the distance of the sample from the decision boundary.\n",
    "\n",
    "The goal of the optimization problem is to minimize the objective function $J(w)$ with respect to the coefficients $w$, subject to the constraints imposed by the regularization term and the loss function. The problem is a convex optimization problem, but it is not strongly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.2.** Fit model ```model_2```  via SGD and complete the following table.\n",
    "\n",
    "\n",
    "| Number of epochs              | Training Accuracy   \t| Test Accuracy \t| Runtime \t    | Ave. Per Epoch Runtime    |\n",
    "|----------------------------\t|--------------------\t|-----------------\t|-------------\t|-----------------------\t|\n",
    "| 6                       \t    |0.86065   \t            |0.8877             |70.6203        |11.770                     |\n",
    "\n",
    "How does the runtime of SGD (in seconds) vary between ```model_1``` and ```model_2```? Do you see that SGD becomes faster when fitting ```model_2``` compared to ```model_1```? Why? \n",
    "Do you see that  ```model_2``` has a higher test accuracy compared to ```model_1```? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    X_train, X_test, y_train, y_test = data_loader()\n",
    "\n",
    "    # Initialize SGDClassifier with appropriate hyperparameters\n",
    "    model_2 = SGDClassifier(loss='log', penalty='l2', alpha=0.001, max_iter=1000, tol=1e-3)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    model_2.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Predict the training and test labels\n",
    "    y_train_pred = model_2.predict(X_train)\n",
    "    y_test_pred = model_2.predict(X_test)\n",
    "\n",
    "    # Compute the training and test accuracies\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Fill out the table\n",
    "    num_epochs = model_2.n_iter_\n",
    "    runtime = end_time - start_time\n",
    "    ave_epoch_runtime = runtime / num_epochs\n",
    "\n",
    "    print(\"Number of epochs:\", num_epochs)\n",
    "    print(\"Training Accuracy:\", train_acc)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "    print(\"Runtime:\", runtime, \"seconds\")\n",
    "    print(\"Ave. Per Epoch Runtime:\", ave_epoch_runtime, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect SGD to become faster when fitting model_2 compared to model_1 because model_2 has a smaller number of parameters. This means that there are fewer updates that need to be made to the model during each epoch, which reduces the overall computation time.\n",
    "It's possible that model_2 has a higher test accuracy compared to model_1. This could be because the regularization imposed by the L2 penalty in model_2 helps to prevent overfitting, or because the simpler model structure of model_2 is better suited to the data. However, without more information it's difficult to say for sure why one model performs better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.3.** Set ```tol=1e-5``` in ```model_2``` (in Parts 4.1 and 4.2, the default value of ```tol``` is ```1e-3``` since we did not specify ```tol```). \n",
    "Fit appropriate variants of ```model_2``` via SGD and complete the following table. Note that for learning rate ```adaptive```, you should set ```eta0=1```. \n",
    "Moreover, the total runtime to complete the following table can be around an hour.\n",
    "\n",
    "| alpha     | learning_rate   \t| Number of epochs \t    | Training Accuracy \t    | Test Accuracy \t| Runtime   \t|  \n",
    "|---------- |-----------------\t|-----------\t        |-------------------------\t|---------------\t|-------------\t|\n",
    "| 1e-4      |               optimal        \t|         10  \t        |           0.8611              \t|       0.8876        \t|       196.9054      \t|                                   \n",
    "| 1e-4      | adaptive        \t|  10         \t        |              0.8611           \t|        0.8876       \t|     190.7078        \t|\n",
    "| 1e-1      | optimal        \t|           \t   10     |       0.8607                  \t|        0.8876       \t|             \t185.8603|                                   \n",
    "| 1e-1      | adaptive        \t|10           \t        |0.8607                        \t|0.8876               \t|190.8612           \t|\n",
    "\n",
    "Do you see any significant change in the training and test accuracies of  ```model_2``` when you change the regularization term's weight (```alpha```) and the learning rate strategy? Compare the runtime of the models\n",
    "in the above table. Please justify your observations.<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "model_2 = SGDClassifier(loss='log', penalty='l2', tol=1e-5, random_state=0)\n",
    "\n",
    "alphas = [1e-4, 1e-1]\n",
    "learning_rates = ['optimal', 'adaptive']\n",
    "\n",
    "table = {'alpha': [], 'learning_rate': [], 'num_epochs': [], 'train_acc': [], 'test_acc': [], 'runtime': []}\n",
    "\n",
    "for alpha in alphas:\n",
    "    for lr in learning_rates:\n",
    "        model_2.alpha = alpha\n",
    "        model_2.learning_rate = lr\n",
    "        start_time = time.time()\n",
    "        train_accs, test_accs = [], []\n",
    "        for epoch in range(10):\n",
    "            model_2.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "            train_acc = accuracy_score(y_train, model_2.predict(X_train))\n",
    "            test_acc = accuracy_score(y_test, model_2.predict(X_test))\n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        table['alpha'].append(alpha)\n",
    "        table['learning_rate'].append(lr)\n",
    "        table['num_epochs'].append(10)\n",
    "        table['train_acc'].append(train_accs[-1])\n",
    "        table['test_acc'].append(test_accs[-1])\n",
    "        table['runtime'].append(runtime)\n",
    "\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(pd.DataFrame(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the table above, it appears that the choice of regularization weight and learning rate strategy does not significantly affect the performance of model_2 in terms of training and test accuracy. The training and test accuracies vary by a very small amount (less than 1%), regardless of the values of alpha and learning_rate. This suggests that the model is relatively robust to changes in these hyperparameters.\n",
    "\n",
    "However, we do observe some differences in the runtime of the models. In general, the adaptive learning rate strategy takes longer to converge than the optimal strategy, which can be seen from the longer runtimes of the adaptive models in the table above. This is because the adaptive strategy adjusts the learning rate on a per-iteration basis, which can be more computationally expensive than the optimal strategy, which uses a fixed learning rate.\n",
    "Overall, based on the results from the table, we can conclude that model_2 is a relatively stable and robust model, which performs well across a wide range of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5.** Recall ```model_2``` from Part (4) with the following parameters:\n",
    "\n",
    "| Parameter | loss    \t| penalty \t| alpha \t| l1_ratio \t| fit_intercept \t| learning_rate \t| eta0 \t| random_state  |\n",
    "|-------\t|---------\t|---------\t|-------\t|----------\t|---------------\t|---------------\t|------\t|------\t        |\n",
    "| Value  \t| 'hinge' \t| 'l2'    \t| 1e-4   \t| 0.0      \t| False          \t| 'constant'    \t| 1.0   | 321\t        |\n",
    "\n",
    "\n",
    "We next study the runtime and the accuracy of SGD when the loss function in ```model_2``` is varied. Fit the following models via SGD and complete the table:\n",
    "\n",
    "| loss              | Number of epochs \t    | Training Accuracy \t    | Test Accuracy \t| Runtime   \t    |   \n",
    "|----------\t        | ----------\t        |-----------\t            |-----------------  |---------------    | \n",
    "| 'hinge'           |                       |                           |                   |                   | \n",
    "| 'squared_hinge'   |                       |                           |                   |                   | \n",
    "| 'log'             |                       |                           |                   |                   | \n",
    "| 'squared_error'   |.                      |.                          |.                  |.                  | \n",
    "    \n",
    "Which loss function gives the best test accuracy? Which one makes SGD converge faster? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "models = {'hinge': SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, l1_ratio=0, fit_intercept=False, \n",
    "                                 learning_rate='constant', eta0=1.0, random_state=321),\n",
    "          'squared_hinge': SGDClassifier(loss='squared_hinge', penalty='l2', alpha=1e-4, l1_ratio=0, fit_intercept=False, \n",
    "                                         learning_rate='constant', eta0=1.0, random_state=321),\n",
    "          'log': SGDClassifier(loss='log', penalty='l2', alpha=1e-4, l1_ratio=0, fit_intercept=False, \n",
    "                               learning_rate='constant', eta0=1.0, random_state=321),\n",
    "          'squared_error': SGDClassifier(loss='squared_error', penalty='l2', alpha=1e-4, l1_ratio=0, fit_intercept=False, \n",
    "                                         learning_rate='constant', eta0=1.0, random_state=321)}\n",
    "\n",
    "table = {'loss': [], 'num_epochs': [], 'train_accuracy': [], 'test_accuracy': [], 'runtime': []}\n",
    "\n",
    "for loss, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    runtime = time.time() - start_time\n",
    "    num_epochs = model.n_iter_\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    table['loss'].append(loss)\n",
    "    table['num_epochs'].append(num_epochs)\n",
    "    table['train_accuracy'].append(train_accuracy)\n",
    "    table['test_accuracy'].append(test_accuracy)\n",
    "    table['runtime'].append(runtime)\n",
    "    \n",
    "print(pd.DataFrame(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that the 'log' and 'squared_hinge' loss functions give the best test accuracy while the 'hinge' loss function performs the worst. The 'squared_hinge' and 'log' loss functions make SGD converge faster due to their suitability for classification problems, while the 'hinge' and 'squared_error' loss functions are more suited for regression problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IDS435-Assignment-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
